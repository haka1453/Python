{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest importance\n",
    "\n",
    "**Random forests** is one of the **most popular** machine learning algorithms. It is **so successful** because it provide **good predictive performance**, **low overfitting** and **easy interpretability**. It is **straightforward to derive the importance of each variable** on the tree decision. In other words, it is easy to compute **how much each variable is contributing to the decision**.\n",
    "\n",
    "**Random forests** consist typically of **4-12 hundred decision trees**, each of them built over a **random extraction** of the **observations** and the **features** from the dataset . **Not every tree sees all the features or all the observations**, and this guarantees that the **trees are de-correlated** and therefore **less prone to over-fitting**. Each tree is also a **sequence of yes-no questions** based on a single or a combination of features. At each node (that is, at each question), the **tree divides the dataset in 2 buckets**, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the **importance of each feature is derived by how \"pure\" each of the buckets is.**\n",
    "\n",
    "For **classification**, **the measure of impurity** is either **Gini** or the **entropy.** For **regression** the  measure of impurity is the **variance**. When training a tree, it is possible to **compute how much each feature decreases the impurity**. The more a **feature decreases the impurity,** the **more important** the feature is. In random forests, the impurity decrease elicited by each feature is averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "In general, +**features that are selected at the top of the trees are more important** than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
    "\n",
    "**Note** Random Forests and decision trees in general give preference to features with **high cardinality.** Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset_2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "Select the features by **examining only the training set** to **avoid overfit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features with tree importance\n",
    "\n",
    "**Fit Random Forests** and **select features** in 2 lines of code! First specify the **Random Forest instance and its parameters**! Then use the **selectFromModel** class from sklearn to **automatically select the features**! **SelectFrom model** will select those features which importance is greater than the **mean importance of all the features** by default, but you can alter this threshold if you want to!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(n_estimators=10,\n",
       "                                                 random_state=10))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=10))\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise those features that were selected! Sklearn will select those features which importance values are greater than the mean of all the coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False, False,  True, False, False,  True,\n",
       "       False, False, False,  True, False,  True,  True,  True, False,\n",
       "       False,  True,  True, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False, False,  True,  True, False, False,  True, False,\n",
       "       False, False,  True, False, False, False, False,  True, False])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a list and count the selected features!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_1', 'var_2', 'var_6', 'var_9', 'var_13', 'var_15', 'var_16',\n",
       "       'var_17', 'var_20', 'var_21', 'var_30', 'var_34', 'var_37', 'var_55',\n",
       "       'var_60', 'var_67', 'var_69', 'var_70', 'var_71', 'var_82', 'var_87',\n",
       "       'var_88', 'var_95', 'var_96', 'var_99', 'var_103', 'var_108'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(sel_.estimator_.feature_importances_.ravel()).hist(bins=20)\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the  amount of selected features with the amount of features which importance is above themean of all features, to make sure we understand the output of SelectFromModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 108\n",
      "selected features: 27\n",
      "features with importance greater than the mean importance of all features: 27\n"
     ]
    }
   ],
   "source": [
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print(\n",
    "    'features with importance greater than the mean importance of all features: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ >\n",
    "               sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If we change the parameters of the tree, we may obtain different features! **How many features** to select is somewhat **arbitrary**! With **SelectFromModel** we use **the mean of all importances as threshold**. We can **modify this threshold** within the SelectFromModel if we want more or less features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('HousingPrices_train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, feature selection should be done **after data pre-processing**, so ideally, **all the categorical variables are encoded into numbers**, and then you can assess **how deterministic** they are of the target! For simplicity, use only **numerical variables**! Select **numerical columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features with tree importance\n",
    "\n",
    "Train a **random forest** for regression and **select features in 2 lines of code**! SelectFrom model will select those features which **importance is greater than the mean importance** of all the features by default, but you can alter this threshold if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestRegressor(random_state=10))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=10))\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a list and count the selected features!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the  amount of selected features with the amount of features which importance is above the mean importance, to make sure we understand the output of sklearn!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 37\n",
      "selected features: 5\n",
      "features with coefficients greater than the mean coefficient: 5\n"
     ]
    }
   ],
   "source": [
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print(\n",
    "    'features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ >\n",
    "               sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features by using tree derived feature importance is a **very straightforward, fast** and generally **accurate way** of selecting good features for machine learning. In particular, if you are going to build tree methods. However, as I said, **correlated features will show in a tree similar importance**, but lower than compared to what their importance would be if the tree was built without the correlated counterparts. In situations like this, it is **better to select features recursively**, rather than altogether."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

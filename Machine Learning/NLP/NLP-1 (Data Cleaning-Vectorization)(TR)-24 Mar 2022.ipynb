{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee05cb3",
   "metadata": {},
   "source": [
    "# Data Cleaning and Vectorization For NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7eb1d",
   "metadata": {},
   "source": [
    "## Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a9a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31e86a",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd21cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fd5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Oh man, this is pretty cool. We will do more such things. 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ab7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ce8965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh man, this is pretty cool.', 'we will do more such things.', '2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_token = sent_tokenize(sample_text.lower())\n",
    "sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e86d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " '.',\n",
       " '2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = word_tokenize(sample_text.lower())\n",
    "word_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3112e28",
   "metadata": {},
   "source": [
    "## Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154ac4f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalpha()] # .isalnum() for number and object\n",
    "tokens_without_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364fe5b",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f3d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\husey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "befecdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20654854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee67c8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b45ec80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'things']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_without_sw = [t for t in tokens_without_punc if t not in stop_words] # if you make a sentiment analysis , you can't remove \n",
    "                                                                           # negative auxiliary verb\n",
    "token_without_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47651f0b",
   "metadata": {},
   "source": [
    "## Data Normalization-Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86dd5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb9c4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb3ffbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drove'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"drove\", pos = \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90f5fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t) for t in token_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb941a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'thing']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94a4b4",
   "metadata": {},
   "source": [
    "## Data Normalization-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5f7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59f2cb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"driving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8924c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = [PorterStemmer().stem(t) for t in token_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b47a859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretti', 'cool', 'thing']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241dbb7",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd60d9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh man pretty cool thing'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "300191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Function - for classification (NOT for sentiment analysis)\n",
    "def cleaning(data):\n",
    "    \n",
    "    #1. Tokenize\n",
    "    text_tokens = word_tokenize(data.lower()) \n",
    "    \n",
    "    #2. Remove Puncs\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "    \n",
    "    #3. Removing Stopwords\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ab10c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oh man pretty cool thing\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sample_text).apply(cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794001d",
   "metadata": {},
   "source": [
    "## Cleaning Function - for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "832956fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Oh man, this is pretty cool. We will do more such things. don't aren't are not. no problem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d5add75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh',\n",
       " 'man',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " '.',\n",
       " 'We',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " '.',\n",
       " 'dont',\n",
       " 'arent',\n",
       " 'are',\n",
       " 'not',\n",
       " '.',\n",
       " 'no',\n",
       " 'problem']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sample_text.replace(\"'\",'')\n",
    "word = word_tokenize(s)\n",
    "word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d099ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"not\", \"no\"]:\n",
    "        stop_words.remove(i)\n",
    "\n",
    "def cleaning_fsa(data):\n",
    "    \n",
    "    #1. removing upper brackets to keep negative auxiliary verbs in text\n",
    "    text = data.replace(\"'\",'')\n",
    "         \n",
    "    #2. Tokenize\n",
    "    text_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #3. Remove numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "    \n",
    "    #4. Removing Stopwords     \n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #5. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f19fc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'nor',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b0cfab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oh man pretty cool thing dont arent not no pro...\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sample_text).apply(cleaning_fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531f547",
   "metadata": {},
   "source": [
    "## CountVectorization and TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "487d0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"airline_tweets.csv\", encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bb55555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ab1d357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment                                               text\n",
       "0               neutral                @VirginAmerica What @dhepburn said.\n",
       "1              positive  @VirginAmerica plus you've added commercials t...\n",
       "2               neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3              negative  @VirginAmerica it's really aggressive to blast...\n",
       "4              negative  @VirginAmerica and it's a really big bad thing...\n",
       "...                 ...                                                ...\n",
       "14635          positive  @AmericanAir thank you we got on a different f...\n",
       "14636          negative  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14637           neutral  @AmericanAir Please bring American Airlines to...\n",
       "14638          negative  @AmericanAir you have my money, you change my ...\n",
       "14639           neutral  @AmericanAir we have 8 ppl so we need 2 know h...\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['airline_sentiment','text']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a6587f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "7           neutral  @VirginAmerica Really missed a prime opportuni..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:8, :]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9115b707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "7           neutral  @VirginAmerica Really missed a prime opportuni..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d414dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aaa6542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus youve added commercial expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica didnt today must mean need take ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica seriously would pay flight seat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica yes nearly every time fly vx go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica really missed prime opportunity ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                        virginamerica dhepburn said\n",
       "1          positive  virginamerica plus youve added commercial expe...\n",
       "2           neutral  virginamerica didnt today must mean need take ...\n",
       "3          negative  virginamerica really aggressive blast obnoxiou...\n",
       "4          negative                 virginamerica really big bad thing\n",
       "5          negative  virginamerica seriously would pay flight seat ...\n",
       "6          positive  virginamerica yes nearly every time fly vx go ...\n",
       "7           neutral  virginamerica really missed prime opportunity ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"text\"] = df2[\"text\"].apply(cleaning_fsa)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47400ae",
   "metadata": {},
   "source": [
    "## CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80c99a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2[\"text\"]\n",
    "y = df2[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "723bac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65bfa08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.5, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8ab2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5a4d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "X_test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4803f613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['another', 'away', 'bad', 'big', 'dhepburn', 'didnt', 'every',\n",
       "       'fly', 'go', 'mean', 'must', 'nearly', 'need', 'really', 'said',\n",
       "       'take', 'thing', 'time', 'today', 'trip', 'virginamerica', 'vx',\n",
       "       'yes'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aabea901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "917d39d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>didnt</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  away  bad  big  dhepburn  didnt  every  fly  go  mean  must  \\\n",
       "0        0     1    0    0         0      0      1    1   1     0     0   \n",
       "1        0     0    0    0         1      0      0    0   0     0     0   \n",
       "2        1     0    0    0         0      1      0    0   0     1     1   \n",
       "3        0     0    1    1         0      0      0    0   0     0     0   \n",
       "\n",
       "   nearly  need  really  said  take  thing  time  today  trip  virginamerica  \\\n",
       "0       1     0       0     0     0      0     1      0     0              1   \n",
       "1       0     0       0     1     0      0     0      0     0              1   \n",
       "2       0     1       0     0     1      0     0      1     1              1   \n",
       "3       0     0       1     0     0      1     0      0     0              1   \n",
       "\n",
       "   vx  yes  \n",
       "0   1    1  \n",
       "1   0    0  \n",
       "2   0    0  \n",
       "3   0    0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count = pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a4773f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    virginamerica yes nearly every time fly vx go ...\n",
       "0                          virginamerica dhepburn said\n",
       "2    virginamerica didnt today must mean need take ...\n",
       "4                   virginamerica really big bad thing\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96e29db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx go away'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cabab33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'virginamerica': 20,\n",
       " 'yes': 22,\n",
       " 'nearly': 11,\n",
       " 'every': 6,\n",
       " 'time': 17,\n",
       " 'fly': 7,\n",
       " 'vx': 21,\n",
       " 'go': 8,\n",
       " 'away': 1,\n",
       " 'dhepburn': 4,\n",
       " 'said': 14,\n",
       " 'didnt': 5,\n",
       " 'today': 18,\n",
       " 'must': 10,\n",
       " 'mean': 9,\n",
       " 'need': 12,\n",
       " 'take': 15,\n",
       " 'another': 0,\n",
       " 'trip': 19,\n",
       " 'really': 13,\n",
       " 'big': 3,\n",
       " 'bad': 2,\n",
       " 'thing': 16}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375a12e2",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b4719",
   "metadata": {},
   "source": [
    "sklearn TD-IDF\n",
    "https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcb3923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "942e866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7450a165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['another', 'away', 'bad', 'big', 'dhepburn', 'didnt', 'every',\n",
       "       'fly', 'go', 'mean', 'must', 'nearly', 'need', 'really', 'said',\n",
       "       'take', 'thing', 'time', 'today', 'trip', 'virginamerica', 'vx',\n",
       "       'yes'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f385e35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.34768534, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34768534, 0.34768534, 0.34768534, 0.        ,\n",
       "        0.        , 0.34768534, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.34768534, 0.        , 0.        ,\n",
       "        0.18143663, 0.34768534, 0.34768534],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.66338461,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.66338461,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34618161, 0.        , 0.        ],\n",
       "       [0.34768534, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34768534, 0.        , 0.        , 0.        , 0.34768534,\n",
       "        0.34768534, 0.        , 0.34768534, 0.        , 0.        ,\n",
       "        0.34768534, 0.        , 0.        , 0.34768534, 0.34768534,\n",
       "        0.18143663, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.48380259, 0.48380259, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.48380259, 0.        ,\n",
       "        0.        , 0.48380259, 0.        , 0.        , 0.        ,\n",
       "        0.25246826, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73e601f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>didnt</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181437</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.347685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.181437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    another      away       bad       big  dhepburn     didnt     every  \\\n",
       "0  0.000000  0.347685  0.000000  0.000000  0.000000  0.000000  0.347685   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.663385  0.000000  0.000000   \n",
       "2  0.347685  0.000000  0.000000  0.000000  0.000000  0.347685  0.000000   \n",
       "3  0.000000  0.000000  0.483803  0.483803  0.000000  0.000000  0.000000   \n",
       "\n",
       "        fly        go      mean      must    nearly      need    really  \\\n",
       "0  0.347685  0.347685  0.000000  0.000000  0.347685  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.347685  0.347685  0.000000  0.347685  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.483803   \n",
       "\n",
       "       said      take     thing      time     today      trip  virginamerica  \\\n",
       "0  0.000000  0.000000  0.000000  0.347685  0.000000  0.000000       0.181437   \n",
       "1  0.663385  0.000000  0.000000  0.000000  0.000000  0.000000       0.346182   \n",
       "2  0.000000  0.347685  0.000000  0.000000  0.347685  0.347685       0.181437   \n",
       "3  0.000000  0.000000  0.483803  0.000000  0.000000  0.000000       0.252468   \n",
       "\n",
       "         vx       yes  \n",
       "0  0.347685  0.347685  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.000000  0.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1219f4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx go away'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c2c450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "really           0.483803\n",
       "bad              0.483803\n",
       "big              0.483803\n",
       "thing            0.483803\n",
       "virginamerica    0.252468\n",
       "another          0.000000\n",
       "vx               0.000000\n",
       "trip             0.000000\n",
       "today            0.000000\n",
       "time             0.000000\n",
       "take             0.000000\n",
       "said             0.000000\n",
       "nearly           0.000000\n",
       "need             0.000000\n",
       "away             0.000000\n",
       "must             0.000000\n",
       "mean             0.000000\n",
       "go               0.000000\n",
       "fly              0.000000\n",
       "every            0.000000\n",
       "didnt            0.000000\n",
       "dhepburn         0.000000\n",
       "yes              0.000000\n",
       "Name: 3, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.loc[3].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b4a1b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>didnt</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  away       bad  big  dhepburn     didnt  every  fly   go  mean  \\\n",
       "0      0.0   0.0  0.000000  0.0       0.0  0.000000    0.0  0.0  0.0   0.0   \n",
       "1      0.0   0.0  0.483803  0.0       0.0  0.483803    0.0  0.0  0.0   0.0   \n",
       "2      0.0   0.0  0.000000  0.0       0.0  0.000000    0.0  0.0  0.0   0.0   \n",
       "3      0.0   0.0  0.000000  0.0       0.0  0.000000    0.0  0.0  0.0   0.0   \n",
       "\n",
       "   must  nearly  need    really  said  take     thing  time  today  trip  \\\n",
       "0   0.0     0.0   0.0  0.886548   0.0   0.0  0.000000   0.0    0.0   0.0   \n",
       "1   0.0     0.0   0.0  0.483803   0.0   0.0  0.483803   0.0    0.0   0.0   \n",
       "2   0.0     0.0   0.0  0.000000   0.0   0.0  0.000000   0.0    0.0   0.0   \n",
       "3   0.0     0.0   0.0  0.886548   0.0   0.0  0.000000   0.0    0.0   0.0   \n",
       "\n",
       "   virginamerica   vx  yes  \n",
       "0       0.462637  0.0  0.0  \n",
       "1       0.252468  0.0  0.0  \n",
       "2       1.000000  0.0  0.0  \n",
       "3       0.462637  0.0  0.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_test_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61028db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica really aggressive blast obnoxious entertainment guest face amp little recourse'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd36ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
